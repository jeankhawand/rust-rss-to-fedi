<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>muffinlabs.com</title>
  <id>http://muffinlabs.com/</id>
  <link href="http://muffinlabs.com/"/>
  <link href="http://muffinlabs.com/atom.xml" rel="self"/>
  <updated>2022-09-10T12:49:00+00:00</updated>
  <author>
    <name>Colin Mitchell</name>
  </author>
  <entry>
    <title>How I maintain botsin.space</title>
    <link rel="alternate" href="http://muffinlabs.com/2022/09/10/how-i-maintain-botsin-space/"/>
    <id>http://muffinlabs.com/2022/09/10/how-i-maintain-botsin-space/</id>
    <published>2022-09-10T12:49:00+00:00</published>
    <updated>2022-09-10T12:49:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">&lt;p&gt;I've been meaning to write up some notes on how I manage &lt;a href="https://botsin.space/"&gt;botsin.space&lt;/a&gt;, and how I've dealt with certain
problems in the past – in particular, the several days of &lt;a href="https://botsin.space/@muffinista/108273247048311451"&gt;issues and
downtime&lt;/a&gt; in May 2022.&lt;/p&gt;

&lt;h2 id="hosting"&gt;Hosting&lt;/h2&gt;

&lt;p&gt;botsin.space is hosted at DigitalOcean. There's nothing really special about DO
(in fact, I think about moving often) but there's a few features that have
really saved me a few times now. First, it's very easy to create a new disk
volume, and once you have a volume, it's pretty easy to expand its size. I store
the database on a separate volume. Currently the database is taking up ~65GB of
space. When the volume is close to full, I'll expand it as needed. Second, it's
very easy to take snapshots of volumes. I have a script that takes a nightly
snapshot of the database volume. I also make snapshots before doing upgrades,
server maintenance, etc. If something bad happens and I need to restore the db
copy, I can create a new volume, attach it to the server, and switch from the
broken db to the snapshot db. I've had to do this several times, and knowing I
can do it again really helps alleviate the stress of running the server.&lt;/p&gt;

&lt;p&gt;I run the instance using &lt;a href="https://github.com/mastodon/mastodon/blob/main/docker-compose.yml"&gt;docker
compose&lt;/a&gt;. I know that docker causes some people a lot of suffering (enough
that the official mastodon documentation doesn't seem to include using docker as
an option anymore), but I like it for a few reasons. First, I have a lot of
professional experience using docker, so I'm used to the different ways it can
cause you pain. Second, I find that using docker makes it a little easier to run
upgrades and rollbacks. Third, it makes it a little easier to maintain the
code/scripts I need to run the instance in git without having to fork the entire
mastodon codebase. Finally, it also makes the service a lot more portable, since
if/when I want to move the instance to a new server, I don't need to reinstall
as many required programs.&lt;/p&gt;

&lt;h2 id="the-code"&gt;The code&lt;/h2&gt;

&lt;p&gt;I have a slightly customized build of mastodon, with a docker file that looks an
awful lot like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM tootsuite/mastodon:v3.5.3

COPY app/views/about/_registration.html.haml /opt/mastodon/app/views/about/
COPY app/views/about/_botsinspace-custom-signup.html.haml /opt/mastodon/app/views/about/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This takes the &lt;a href="https://hub.docker.com/r/tootsuite/mastodon/#!"&gt;pre-existing image&lt;/a&gt; for mastodon, copies a few customized files in, and that's it!&lt;/p&gt;

&lt;p&gt;Here's what the Dockerfile and docker-compose.yml look like:&lt;/p&gt;

&lt;script src="https://gist.github.com/muffinista/b7674bd5afe5d68089a92fb034d72c9e.js"&gt;&lt;/script&gt;

&lt;h2 id="upgrades-and-maintenance"&gt;Upgrades and maintenance&lt;/h2&gt;

&lt;p&gt;When it's time to run an upgrade, I make a snapshot of the database, update the
version numbers in docker-compose.yml, and run something along the lines of
&lt;code&gt;docker compose build &amp;amp;&amp;amp; docker compose up -d&lt;/code&gt;. This builds a new docker image
and deploys it, then restarts everything as needed. If something goes wrong, I
roll back the version and re-run &lt;code&gt;docker compose up -d&lt;/code&gt;. The configuration file
itself could be a little more optimized (ideally I'd only specify the version
stuff once), but I'm lazy and usually do it via search/replace in my editor.&lt;/p&gt;

&lt;h2 id="other-stuff"&gt;Other stuff&lt;/h2&gt;

&lt;p&gt;A few things happen outside of docker:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;nginx&lt;/strong&gt; - nginx runs directly on the server, and routes traffic to docker. The
configuration is reasonably close to the default mastodon configuration file.
There's a couple of rules in there to block some bad actors, and there's some
rate-limiting as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lets Encrypt&lt;/strong&gt; - I use Let's Encrypt to setup HTTPS certificates/etc. I use
DNS validation since there's a special plugin that handles everything via the
Digital Ocean API.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scheduled tasks&lt;/strong&gt; - There's a few nightly tasks running in cron – making
backups, running mastodon maintenance/etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File storage&lt;/strong&gt; - File storage is a huge chunk of the expense of running the
instance. Uploads are stored in Digital Ocean's Spaces, which is basically a
clone of S3. I kept files on S3 for awhile, but I don't like giving Amazon
money, and Spaces is a little cheaper. It's also probably better for performance
to have the file storage closer to the actual server.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Emails&lt;/strong&gt; - Emails are sent with &lt;a href="https://mailpace.com/"&gt;MailPace&lt;/a&gt; and it works
well enough that I basically never think about it.&lt;/p&gt;

&lt;h2 id="server-upgrades"&gt;Server upgrades&lt;/h2&gt;

&lt;p&gt;The botsin.space server is running Ubuntu. Server updates aren't too much of a
concern, but if I need to do upgrade between major versions or something else
large like that, I take advantage of the fact that the database is on its own
dedicated volume. I can boot an entirely new server, install any required
software (I basically have a script for this), copy over my configuration files,
then detach the volume from the old server, attach it to the new one, and update
DNS to point to the new server.&lt;/p&gt;

&lt;h2 id="moderation-and-new-accounts"&gt;Moderation and new accounts&lt;/h2&gt;

&lt;p&gt;At the moment, I handle all moderation issues and new account requests myself. I
use a slightly tweaked version of &lt;a href="https://github.com/bclindner/ivory"&gt;ivory&lt;/a&gt; to help with spam signups
and things like that. It's certainly possible that this will become enough work
that I can't handle it myself, but that hasn't happened yet.&lt;/p&gt;

&lt;h2 id="when-things-go-wrong"&gt;When things go wrong&lt;/h2&gt;

&lt;p&gt;The upgrade to Mastodon v3.5.0 involved upgrading PostgreSQL from version 9.6 to
14. There were instructions for running this upgrade that were along the lines
of: make a dump of the data in the old version of postgres, upgrade, then import
the data into the new version. With a large database, that can take hours or
even days, and if it fails while it's running, that's a bunch of time that
you've wasted. So, I took a snapshot and shutdown botsin.space, and started
running the dump. Unfortunately, the process failed for me over and over again,
and when I eventually got it to work, and tried to bring botsin.space back
online, it was clear that there were some data issues. I rolled back to the old
snapshot and started running upgrade tests on a separate test server.&lt;/p&gt;

&lt;p&gt;Eventually I found a &lt;a href="https://github.com/tianon/docker-postgres-upgrade"&gt;neat little docker
image&lt;/a&gt; that can be used to upgrade between postgres versions, and that seemed
to work.&lt;/p&gt;

&lt;p&gt;However, there was another problem – botsin.space was experiencing a &lt;a href="https://docs.joinmastodon.org/admin/troubleshooting/index-corruption/"&gt;data
corruption issue&lt;/a&gt;. When I tried to run mastodon's custom &lt;a href="https://github.com/mastodon/mastodon/blob/b07906bdb0127cd73662506b519183cc51a2758e/lib/mastodon/maintenance_cli.rb#L139"&gt;fix-duplicates&lt;/a&gt;
script, I found a whole new set of issues. That script checks a bunch of tables
for duplicate data. Many of those tables have a manageable amount of data in
them, but some of them – particularly the conversations and status tables –
each of which have over 50 million rows in them right now. The script was trying
to run fairly complicated queries against that table, but the server didn't have
enough memory to process the result. This meant I needed to write some custom
ruby code to do the same thing without causing quite so much server load. I
managed to do that (luckily I program in Ruby for a living), let it run for a
couple of hours, and when it was done, I was able to bring botsin.space back
online.&lt;/p&gt;

&lt;p&gt;If I hadn't been able to take snapshots, and increase the database storage
volume as needed, and if I wasn't well-versed in Ruby, there's a good chance
that this upgrade would've either failed entirely, involved a lot of data lostt,
or taken many days/weeks to finish.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>SpaceJamCheck in the New York Times</title>
    <link rel="alternate" href="https://www.nytimes.com/2021/05/21/style/welcome-to-the-space-jam-again.html"/>
    <id>https://www.nytimes.com/2021/05/21/style/welcome-to-the-space-jam-again.html</id>
    <published>2021-05-24T00:00:00+00:00</published>
    <updated>2021-05-24T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I was interviewed in the NYT as part of an article about the Space Jam website and my Twitter bot @SpaceJamCheck.</content>
  </entry>
  <entry>
    <title>Emoji Fireplace app</title>
    <link rel="alternate" href="https://muffinlabs.com/emoji-fireplace"/>
    <id>https://muffinlabs.com/emoji-fireplace</id>
    <published>2019-12-29T00:00:00+00:00</published>
    <updated>2019-12-29T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made an app version of my emoji yule log for Android devices.</content>
  </entry>
  <entry>
    <title>Audio Sweetener Bot</title>
    <link rel="alternate" href="https://botsin.space/@audiosweetener"/>
    <id>https://botsin.space/@audiosweetener</id>
    <published>2019-01-10T00:00:00+00:00</published>
    <updated>2019-01-10T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">Here's a bot that posts audio clips from the BBC Sound Effects Archive.</content>
  </entry>
  <entry>
    <title>Tall Boy</title>
    <link rel="alternate" href="https://glitch.com/~tall-boy"/>
    <id>https://glitch.com/~tall-boy</id>
    <published>2018-11-16T00:00:00+00:00</published>
    <updated>2018-11-16T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made a p5.js sketch that is a rendition of those inflatable dancers you see in front of stores and in other random places</content>
  </entry>
  <entry>
    <title>November Rain Bot</title>
    <link rel="alternate" href="https://botsin.space/@NovemberRain"/>
    <id>https://botsin.space/@NovemberRain</id>
    <published>2018-11-16T00:00:00+00:00</published>
    <updated>2018-11-16T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made a bot that posts frames from the November Rain video. It'll only run for the month of November. It runs in rough order, so the bot starts at the start of the video, and it should end on the last frame.</content>
  </entry>
  <entry>
    <title>Pitchers and Catchers</title>
    <link rel="alternate" href="http://pitchersandcatchersreport.in/"/>
    <id>http://pitchersandcatchersreport.in/</id>
    <published>2018-10-29T00:00:00+00:00</published>
    <updated>2018-10-29T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made a website to let you know when Spring Training starts</content>
  </entry>
  <entry>
    <title>The Secret Broadcast</title>
    <link rel="alternate" href="https://secretbroadcast.net/"/>
    <id>https://secretbroadcast.net/</id>
    <published>2018-09-18T00:00:00+00:00</published>
    <updated>2018-09-18T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">The Secret Broadcast is a numbers station podcast. The content of the podcast is encrypted messages. You can submit a message and it will be read as an encrypted sequence of letters and numbers, and which can be decrypted with a key only available to you and anyone you share it with.</content>
  </entry>
  <entry>
    <title>Emily Dickinson's Herbarium</title>
    <link rel="alternate" href="https://botsin.space/@herbarium"/>
    <id>https://botsin.space/@herbarium</id>
    <published>2018-09-14T00:00:00+00:00</published>
    <updated>2018-09-14T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made a bot that posts images from Emily Dickinson's collection of pressed flowers and botanical samples.</content>
  </entry>
  <entry>
    <title>moire</title>
    <link rel="alternate" href="https://glitch.com/~p5-moireish-color"/>
    <id>https://glitch.com/~p5-moireish-color</id>
    <published>2018-04-10T00:00:00+00:00</published>
    <updated>2018-04-10T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made another screensaver on glitch before adding it to Before Dawn, it's a neat experiment with moire patterns and colors.</content>
  </entry>
  <entry>
    <title>p5.js fullscreen starter on glitch</title>
    <link rel="alternate" href="https://p5-fullerscreen-starter.glitch.me/"/>
    <id>https://p5-fullerscreen-starter.glitch.me/</id>
    <published>2018-04-06T00:00:00+00:00</published>
    <updated>2018-04-06T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made a p5.js starter project on glitch with a full screen toggle. Then I used that project to write a version of the old &lt;a href='https://before-dawn-mystify.glitch.me'&gt;mystify&lt;/a&gt; screensaver.</content>
  </entry>
  <entry>
    <title>Before Dawn v0.9.25</title>
    <link rel="alternate" href="https://github.com/muffinista/before-dawn/releases"/>
    <id>https://github.com/muffinista/before-dawn/releases</id>
    <published>2018-04-06T00:00:00+00:00</published>
    <updated>2018-04-06T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">Before Dawn has been updated with a couple new features and new screensavers. In particular, you can specify that you only want to run screensavers on a single monitor -- which is a handy way to keep your CPU load lower.</content>
  </entry>
  <entry>
    <title>Before Dawn v0.9.14</title>
    <link rel="alternate" href="https://github.com/muffinista/before-dawn/releases"/>
    <id>https://github.com/muffinista/before-dawn/releases</id>
    <published>2017-12-12T00:00:00+00:00</published>
    <updated>2017-12-12T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I rewrote Before Dawn to use Vue.js, fixed a pile of bugs, and improved perfomance.</content>
  </entry>
  <entry>
    <title>Nice Gradients</title>
    <link rel="alternate" href="http://muffinlabs.com/gradients/"/>
    <id>http://muffinlabs.com/gradients/</id>
    <published>2017-11-17T00:00:00+00:00</published>
    <updated>2017-11-17T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made a screensaver that slowly updates color gradients on your screen. It's part of &lt;a href='https://github.com/muffinista/before-dawn'&gt;Before Dawn&lt;/a&gt; but I also wanted to put a copy of it on my website.</content>
  </entry>
  <entry>
    <title>Before Dawn v0.9.11</title>
    <link rel="alternate" href="https://github.com/muffinista/before-dawn/releases"/>
    <id>https://github.com/muffinista/before-dawn/releases</id>
    <published>2017-11-16T00:00:00+00:00</published>
    <updated>2017-11-16T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I continue to plug away at Before Dawn. It is getting very stable and usuable now. I've added a bunch of screensavers and have plans to add even more.</content>
  </entry>
  <entry>
    <title>Editorialize Chrome Plugin</title>
    <link rel="alternate" href="https://chrome.google.com/webstore/detail/editorialize/emfpglodamcbfnecphcmlkbhnlloghko"/>
    <id>https://chrome.google.com/webstore/detail/editorialize/emfpglodamcbfnecphcmlkbhnlloghko</id>
    <published>2017-08-30T00:00:00+00:00</published>
    <updated>2017-08-30T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made a simple Chrome plugin which replaces any text on a NY Times editorial with poop emoji. The source code is &lt;a href='https://github.com/muffinista/editorialize'&gt;on github&lt;/a&gt;</content>
  </entry>
  <entry>
    <title>Lonely Computer</title>
    <link rel="alternate" href="http://muffinlabs.com/screensavers/4-listening/"/>
    <id>http://muffinlabs.com/screensavers/4-listening/</id>
    <published>2017-08-06T00:00:00+00:00</published>
    <updated>2017-08-06T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I wrote about a screensaver that reacts to noise.</content>
  </entry>
  <entry>
    <title>Defrag</title>
    <link rel="alternate" href="http://muffinlabs.com/screensavers/3-defragment/"/>
    <id>http://muffinlabs.com/screensavers/3-defragment/</id>
    <published>2017-07-28T00:00:00+00:00</published>
    <updated>2017-07-28T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I wrote a little bit about a screensaver that defragments your screen.</content>
  </entry>
  <entry>
    <title>Atari Attract Mode</title>
    <link rel="alternate" href="http://muffinista.github.io/before-dawn-screensavers/#atari-attract-mode"/>
    <id>http://muffinista.github.io/before-dawn-screensavers/#atari-attract-mode</id>
    <published>2017-07-14T00:00:00+00:00</published>
    <updated>2017-07-14T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I added an Atari Attract Mode to Before Dawn. It cycles the screen through low luminosity colors.</content>
  </entry>
  <entry>
    <title>@IndyDaySpeech</title>
    <link rel="alternate" href="https://twitter.com/IndyDaySpeech"/>
    <id>https://twitter.com/IndyDaySpeech</id>
    <published>2017-07-04T00:00:00+00:00</published>
    <updated>2017-07-04T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made a bot that tweets the speech from Independence Day every 4th of July. The source code is &lt;a href='https://github.com/muffinista/indyday'&gt;available on github&lt;/a&gt;</content>
  </entry>
  <entry>
    <title>@eliza on mastodon</title>
    <link rel="alternate" href="https://botsin.space/@eliza"/>
    <id>https://botsin.space/@eliza</id>
    <published>2017-04-20T00:00:00+00:00</published>
    <updated>2017-04-20T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made another bot for Mastodon, this one is a chatbot version of ELIZA</content>
  </entry>
  <entry>
    <title>@loveletter on mastodon</title>
    <link rel="alternate" href="https://botsin.space/@loveletter"/>
    <id>https://botsin.space/@loveletter</id>
    <published>2017-04-05T00:00:00+00:00</published>
    <updated>2017-04-05T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I've been exploring Mastodon as a social network so naturally I made a bot for it. I'm running a bot-friendly instance at &lt;a href='https://botsin.space/'&gt;botsin.space&lt;/a&gt; too.</content>
  </entry>
  <entry>
    <title>The Journey of EarthRoverBot</title>
    <link rel="alternate" href="http://muffinlabs.com/2017/03/28/the-journey-of-earth-rover-bot/"/>
    <id>http://muffinlabs.com/2017/03/28/the-journey-of-earth-rover-bot/</id>
    <published>2017-03-28T16:20:00+00:00</published>
    <updated>2017-03-28T16:20:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">&lt;p&gt;&lt;img src="/2017/images/trip-full.jpg" class="full-bleed" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://twitter.com/earthroverbot"&gt;@EarthRoverBot&lt;/a&gt; is in the final
stretch of a journey from the edge of Maine to the US/Mexico border.
The bot is entirely virtual, and the trip is powered by Google's
Street View data. It takes a step forward every 12 minutes. It has a
location and a bearing, and if there's valid Street View data in that
direction, then it moves forward. If there isn't data, it adjusts
course until it finds a way to continue. With each step it sends the
image to Twitter.&lt;/p&gt;

&lt;p&gt;The bot can be controlled via commands sent through tweets, but mostly
it runs on autopilot, with a simple algorithm that it uses to work its
way towards the border crossing in San Diego. At some point in the
next few weeks, the bot will send an image that looks something like
this:&lt;/p&gt;

&lt;p&gt;&lt;img src="/2017/images/end-view.png" /&gt;&lt;/p&gt;

&lt;p&gt;And the trip will be done.&lt;/p&gt;

&lt;p&gt;Of all the bots I've made, I think this one is my favorite. I love
the experience of a slow, meditative journey, without using a map,
getting stuck in unusual places, finding dead-ends and the insides of
buildings in places where the data is weird.&lt;/p&gt;

&lt;p&gt;Also, while the bot is basically automated, it can accept human
commands, so people have been able to control the course of the bot.
In fact, it never would have made it as far as it has without help
from people.&lt;/p&gt;

&lt;p&gt;At the same time, thanks to the use of Google Street view, the journey
represents a fairly bizarre version of a road trip. Everything that
you are able to see has been dictated by largely commercial needs of a
gigantic company. It's almost always sunny in the world presented by
Street View, although sometimes seasons will change without warning.
There's very little traffic, you never see an accident or weather. The
trip is largely devoid of visible people. The quality of light is
almost constant – it's always the middle of the day and the sun is
usually out. Over days or weeks, the color palette changes in subtle
ways.&lt;/p&gt;

&lt;p&gt;&lt;img src="/2017/images/flatten.gif" /&gt;&lt;/p&gt;

&lt;p&gt;When I made the bot, sending it from one corner of the country to the
other seemed like a fun and fairly innocuous idea, but it spent an
entire election season barreling towards a border that defined so much
of the election, and now it's impossible to avoid the feeling that
driving something towards a destination like this is inherently
political.&lt;/p&gt;

&lt;p&gt;Here's a moment with a collection of images from the journey:&lt;/p&gt;

&lt;p&gt;&lt;a class="twitter-moment" href="https://twitter.com/i/moments/845805228364648448"&gt;EarthRoverBot&lt;/a&gt;
&lt;script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Here's a map of the final leg of the trip:&lt;/p&gt;

&lt;p&gt;&lt;img src="/2017/images/rover-end-map.png" /&gt;&lt;/p&gt;

&lt;p&gt;(it's already moved past this point since I'm the worst blogger ever lol)&lt;/p&gt;

&lt;p&gt;I do have plans for the bot after it has finished this trip. I might
add the ability to jump to specific locations, or I might just start
another trip between two other points. I've thought about making a web
version, and I'll definitely release the source code for the bot.&lt;/p&gt;

&lt;p&gt;Here's a video of the trip of the bot. I took every image is posted to
Twitter, filtered out ones where the bot moved fewer than 10 meters,
then composited them down to a few thousand frames, then turned that
into a video. I'm still experimenting, so I might come up with
something more interesting in the future.&lt;/p&gt;

&lt;iframe width="560" height="400" src="https://www.youtube.com/embed/unow3_ipkmQ?rel=0" frameborder="0" allowfullscreen=""&gt;&lt;/iframe&gt;

</content>
  </entry>
  <entry>
    <title>Before Dawn</title>
    <link rel="alternate" href="https://github.com/muffinista/before-dawn"/>
    <id>https://github.com/muffinista/before-dawn</id>
    <published>2017-02-28T00:00:00+00:00</published>
    <updated>2017-02-28T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made a screensaver tool called Before Dawn! I &lt;a href='/screensavers/'&gt;wrote a little about screensavers&lt;/a&gt; too.</content>
  </entry>
  <entry>
    <title>Trump Administration Twitter Archives</title>
    <link rel="alternate" href="https://archive.org/download/twitterArchiveDumps"/>
    <id>https://archive.org/download/twitterArchiveDumps</id>
    <published>2017-02-12T00:00:00+00:00</published>
    <updated>2017-02-12T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I wrote some code to generate archives of realDonaldTrump tweets, along with several Trump admin accounts. The archive should be constantly updated any time someone tweets. I made this before realizing that the &lt;a href='http://trumptwitterarchive.com/'&gt;trump twitter archive&lt;/a&gt; has made its &lt;a href='https://github.com/bpb27/political_twitter_archive'&gt;data available on github&lt;/a&gt; but it might still be handy.</content>
  </entry>
  <entry>
    <title>buzzcut</title>
    <link rel="alternate" href="https://raw.githubusercontent.com/muffinista/buzzcut/master/output-2.txt"/>
    <id>https://raw.githubusercontent.com/muffinista/buzzcut/master/output-2.txt</id>
    <published>2016-11-30T00:00:00+00:00</published>
    <updated>2016-11-30T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">This was my entry for the 2016 #NaNoGenMo -- a novel generated using cutup techniques on pages from buzzfeed.</content>
  </entry>
  <entry>
    <title>@drillify_exe</title>
    <link rel="alternate" href="https://twitter.com/drillify_exe"/>
    <id>https://twitter.com/drillify_exe</id>
    <published>2016-11-18T00:00:00+00:00</published>
    <updated>2016-11-18T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">@drillify_exe is a bot that juxtaposes a random tweet with a random dril tweet. The source code is &lt;a href='https://github.com/muffinista/drillify_exe'&gt;here&lt;/a&gt;.</content>
  </entry>
  <entry>
    <title>@happened_today</title>
    <link rel="alternate" href="https://twitter.com/happened_today"/>
    <id>https://twitter.com/happened_today</id>
    <published>2016-11-04T00:00:00+00:00</published>
    <updated>2016-11-04T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I updated @happened_today to include images from wikipedia when possible. The source code is &lt;a href='https://github.com/muffinista/happened_today'&gt;on github&lt;/a&gt;.</content>
  </entry>
  <entry>
    <title>@muffin_exe_sta</title>
    <link rel="alternate" href="https://twitter.com/muffin_exe_sta"/>
    <id>https://twitter.com/muffin_exe_sta</id>
    <published>2016-11-04T00:00:00+00:00</published>
    <updated>2016-11-04T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">I made an ebooks-style bot that glitches old pictures from my main Twitter account.</content>
  </entry>
  <entry>
    <title>@head_2_keyboard</title>
    <link rel="alternate" href="https://twitter.com/head_2_keyboard"/>
    <id>https://twitter.com/head_2_keyboard</id>
    <published>2016-11-04T00:00:00+00:00</published>
    <updated>2016-11-04T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">@head_2_keyboard is an ultra-realistic simulation of smashing a head into a keyboard. &lt;a href='https://github.com/muffinista/head_2_keyboard'&gt;here is the source code&lt;/a&gt;.</content>
  </entry>
  <entry>
    <title>The Making of @lists_of_lists</title>
    <link rel="alternate" href="http://muffinlabs.com/2016/08/16/the-making-of-lists-of-lists/"/>
    <id>http://muffinlabs.com/2016/08/16/the-making-of-lists-of-lists/</id>
    <published>2016-08-16T18:37:00+00:00</published>
    <updated>2016-08-16T18:37:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">&lt;p&gt;I thought I'd write something about how I made the bot
&lt;a href="https://twitter.com/lists_of_lists"&gt;@lists_of_lists&lt;/a&gt;, from start to
finish. It's a relatively simple idea, so if you're interested in
writing a bot for the first time, this might be a helpful guide.&lt;/p&gt;

&lt;p&gt;I have a bit of an advantage for two reasons. First, I'm a
professional programmer, and have been for many years. I know ruby
very well, and it's the language I use to build most of my bots.
Second, I wrote the
&lt;a href="https://github.com/muffinista/chatterbot"&gt;library&lt;/a&gt;, that I use to
make most of my bots, so it's basically adapted to my needs.&lt;/p&gt;

&lt;p&gt;That said, if you are not a developer, but want to make a bot, you
definitely can, but you should probably expect to have to learn a
little bit about coding, and also a little bit about server
management, because getting your bot to run consistently is sometimes
the hardest part of the process.&lt;/p&gt;

&lt;h2 id="the-idea"&gt;The Idea&lt;/h2&gt;

&lt;p&gt;I spent a lot of time exploring wikipedia's data downloads when I was
building &lt;a href="http://gopherpedia.com/"&gt;gopherpedia&lt;/a&gt;. I knew that there
were a lot of 'list of' pages, and that some of them were
&lt;a href="https://en.wikipedia.org/wiki/List_of_salads"&gt;amusing&lt;/a&gt; and
interesting. I decided to see if I could download a list of them so
that I could play around with the data.&lt;/p&gt;

&lt;p&gt;Wikipedia offers database dumps at
&lt;a href="https://dumps.wikimedia.org/"&gt;https://dumps.wikimedia.org/&lt;/a&gt;. The main
files here are gigantic XML files that represent the complete contents
of the website. Depending on what you are interested in, some of these
XML files are 12GB or larger. That's a single XML file! Parsing those
is a real challenge.&lt;/p&gt;

&lt;p&gt;Luckily, they offer a much smaller file of just page titles. I
downloaded that file, and searched it for pages with the words 'list of'
or 'lists of' in the title. I ended up running this a few times, so I
combined it all into a single shell command that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-all-titles-in-ns0.gz &amp;gt; enwiki-latest-all-titles-in-ns0.gz &amp;amp;&amp;amp; gzcat enwiki-latest-all-titles-in-ns0.gz | grep -i 'List_of\|Lists_of' &amp;gt; lists.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At that point, I had a text file that looked a little like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;"List_of_the_works_of_Charles_Cottet_depicting_scenes_of_Brittany
"List_of_the_works_of_Charles_Cottet_depicting_scenes_of_Brittany"
'List_of_Mongolian_musical_instruments
(List_of_Toni,la_Chef_episodes)
/List_of_Parliament_of_Australia_Reports_on_Sport
1996_World_Monuments_Fund_List_of_Most_Endangered_Sites
1996_World_Monuments_Watch_List_of_Most_Endangered_Sites
1998_World_Monuments_Fund_List_of_Most_Endangered_Sites
1998_World_Monuments_Watch_List_of_Most_Endangered_Sites
2000_World_Monuments_Fund_List_of_Most_Endangered_Sites
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="sit-on-it-for-a-year"&gt;Sit on it for a year&lt;/h2&gt;

&lt;p&gt;Once I had the data, I had no idea what I actually wanted to do with
it. I thought about running it through a Markov chain tool, or
maybe swapping out words randomly, adding adjectives and modifiers,
etc, etc.&lt;/p&gt;

&lt;p&gt;I couldn't really decide what to do, so I didn't do anything. I let
the data sit around for a year or so.&lt;/p&gt;

&lt;p&gt;Eventually, I decided to just keep it simple and make a bot that would
simply iterate through the list of lists. I randomized the data to
make it a little more interesting:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gshuf lists.txt &amp;gt; lists-random.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(gshuf is an OSX command to randomly shuffle the lines of a file. If
it's not installed already, you can install it via &lt;code&gt;brew install
coreutils&lt;/code&gt;. On Linux, there's a command called &lt;code&gt;shuf&lt;/code&gt; that does the
exact same thing. I suspect it's pre-installed on most Linux systems.
Thanks to &lt;a href="https://twitter.com/ckolderup"&gt;@ckolderup&lt;/a&gt; for pointing
all of this out!&lt;/p&gt;

&lt;h2 id="start-the-bot"&gt;Start The bot&lt;/h2&gt;

&lt;p&gt;I had the data, now I needed the bot. Amazingly, when I went to
Twitter to register a new account, my first choice was available, so
&lt;a href="https://twitter.com/lists_of_lists"&gt;@lists_of_lists&lt;/a&gt; was born.&lt;/p&gt;

&lt;p&gt;I made myself a directory to hold onto my bot files, and copied the
data there. Then, I setup a &lt;code&gt;Gemfile&lt;/code&gt; and got ready to install
&lt;code&gt;chatterbot&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir lists_of_lists
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I made a Gemfile that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source "https://rubygems.org"
gem "chatterbot", :git =&amp;gt; "git://github.com/muffinista/chatterbot.git"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I ran &lt;code&gt;bundle&lt;/code&gt; to install chatterbot.&lt;/p&gt;

&lt;p&gt;Chatterbot has a script which will walk you through the process of
setting up a Twitter bot. It will also create a template file for the
bot, and setup your credentials file. I ran it!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; I ran all of this while being logged into Twitter as the
account for the bot.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bundle exec chatterbot-register
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It prints out a message telling me what happens next:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Welcome to Chatterbot. Let's walk through the steps to get a bot running.

Hey, looks like you need to get an API key from Twitter before you can get started.

Have you already set up an app with Twitter? [Y/N]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I haven't setup an app yet, so I put 'N'&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; N
OK, I can help with that!

Please hit enter, and I will send you to https://apps.twitter.com/app/new to start the process.
(If it doesn't work, you can open a browser and paste the URL in manually)

Hit Enter to continue.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The form looked a lot like this (they change this a lot):&lt;/p&gt;

&lt;p&gt;&lt;img src="http://muffinlabs.com/images/twitter-app-signup.png" alt="Twitter App Form" title="Twitter App Form" /&gt;&lt;/p&gt;

&lt;p&gt;Once you've filled out that form, Twitter will issue you some API
keys. I copied those keys into chatterbot-register, which was waiting
for the input:&lt;/p&gt;

&lt;p&gt;&lt;img src="http://muffinlabs.com/images/twitter-app-settings.png" alt="Twitter App Settings" title="Twitter App Settings" /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Once you've filled out the app form, click on the 'Keys and Access Tokens' link


Paste the 'Consumer Key' here: 123456
Paste the 'Consumer Secret' here: abcdefg

Now it's time to authorize your bot!

Do you want to authorize a bot using the account that created the app? [Y/N]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I do want to authorize this account, so I say so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; Y
OK, on the app page, you can click the 'Create my access token' button
to proceed.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I do that, then I paste the results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Paste the 'Access Token' here: 123456

Paste the 'Access Token Secret' here: 45678
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hooray, now I have two files! lists_of_lists.rb is a template file for
my bot. It lists a bunch of features of chatterbot and gives you
something to work from. lists_of_lists.yml has the credentials for the
bot, and will also track some other information needed to send out
tweets.&lt;/p&gt;

&lt;p&gt;My idea for the bot is pretty simple. Each time it runs, it should
open up the file with all the lists in it, read the next one, and
tweet it out.&lt;/p&gt;

&lt;p&gt;The bot will need to keep track of which line it sent
out last, and update that value every time. One of the features of
chatterbot is that the YAML file which holds the configuration data is
accessible to the bot, and is updated with any changes each time the
bot is run. This means you can use it to track variables that you need
to persist over time, such as the last index of a file that you used.&lt;/p&gt;

&lt;p&gt;So I start with some ruby to handle all of that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SOURCE = "lines.txt"

bot.config[:index] ||= 0

if ENV["FORCE_INDEX"]
  bot.config[:index] = ENV["FORCE_INDEX"].to_i
end
 
data = File.read(SOURCE).split(/\n/)

source = data[ bot.config[:index] ]
puts source

# the page title will have underscores in it, get rid of those
tweet_text = source.gsub(/_/, " ")
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code sets the index variable, opens the file "lines.txt", turns
it into an array by splitting on newlines, and then reads the proper
value from that array.&lt;/p&gt;

&lt;h2 id="make-it-nicer"&gt;Make it Nicer&lt;/h2&gt;

&lt;p&gt;At this point, I could just tweet that value out like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tweet tweet_text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And be done. I decided that would be a little boring though, and I
started to wonder about pulling an image from the wikipedia page for
the list. Some lists have images on them, and they can be
&lt;a href="https://en.wikipedia.org/wiki/List_of_salads"&gt;pretty funny&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Wikipedia has an API, and there are a few ruby libraries for accessing
it. I decided to check out the
&lt;a href="https://github.com/kenpratt/wikipedia-client"&gt;official client&lt;/a&gt; since
I had never used it before. My assumption was that I would need to
parse out images from the source text, but it turns out that there is
a method you can use to get a list of images! Anyway, here's that code&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;page = Wikipedia.find( source )

opts = {}

# check if there are any images
if page.image_urls &amp;amp;&amp;amp; ! page.image_urls.empty?
  puts page.image_urls.inspect

  # pick an image at random
  image_url = filter_images(page.image_urls).sample
  
  puts image_url
  if image_url &amp;amp;&amp;amp; image_url != ""
    # make a local copy of the image
    opts[:media] = save_to_tempfile(image_url)
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I added a simple method &lt;code&gt;filter_images&lt;/code&gt; which rejects any SVG files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def filter_images(list)
  list.reject { |l| l =~ /.svg$/ }
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And a second method &lt;code&gt;save_to_tempfile&lt;/code&gt; which makes a local copy of the
image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def save_to_tempfile(url)
  uri = URI.parse(url)
  ext = [".", uri.path.split(/\./).last].join("")

  dest = File.join "/tmp", Dir::Tmpname.make_tmpname(['list', ext], nil)

  puts "#{url} -&amp;gt; #{dest}"

  open(dest, 'wb') do |file|
    file &amp;lt;&amp;lt; open(url).read
  end

  # if the image is too big, let's lower the quality a bit
  if File.size(dest) &amp;gt; 5_000_000
    `mogrify -quality 65% #{dest}`
  end

  dest
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This method has one additional twist, which is that it checks the size
of the downloaded file. If it's too large, it runs the ImageMagick
command &lt;code&gt;mogrify&lt;/code&gt; on it to drop the quality down.&lt;/p&gt;

&lt;p&gt;At this point, I have the text of a tweet, a &lt;code&gt;page&lt;/code&gt; object from the
Wikiedpedia API library, and a hash that might have a file in it. I
combine it all together and tweet it out:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;output = [ tweet_title, page.fullurl ].join("\n")

begin
  tweet(output, opts)
rescue Exception =&amp;gt; e
  puts e.inspect
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, I increment the index variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bot.config[:index] += 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the script is done running, this value will be updated in the
YAML config file for the bot.&lt;/p&gt;

&lt;p&gt;During this whole process, I ran the script a couple times. Chatterbot
has a &lt;code&gt;debug_mode&lt;/code&gt; command, which you can use to run a script without
actually sending a tweet, which is pretty handy.&lt;/p&gt;

&lt;p&gt;I'm a pretty messy coder, especially when I'm working on personal side
projects, so I fixed a couple bugs, spent awhile cleaning up my junky
code, etc, etc. Once I was happy with it, I uploaded my code to the
server where I run my bots.&lt;/p&gt;

&lt;p&gt;Then I needed to setup a cron job to run the bot every few hours. I
decided to run the bot every two hours for starters (I might slow it
down later), and for variery I run it at 2 minutes past the hour. This
is what the job looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2 */2 * * * . ~/.bash_profile; cd /var/stuff/lists_of_lists/; bundle exec ./lists_of_lists.rb &amp;gt;&amp;gt; tweets.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first bit specifies when the job runs. The rest of it is the
command that executes the bot. cron jobs usually run in a different
environment then you get when you login to a server via SSH, so you
need to explicitly load your environment, cd into the directory where
the script is, and run the script. the &lt;code&gt;&amp;gt;&amp;gt; tweets.log 2&amp;gt;&amp;amp;1&lt;/code&gt; bit sends any output
into the tweets.log file, which I can check for any errors/etc.&lt;/p&gt;

&lt;p&gt;Anyway, that's about it! I've put the code &lt;a href="https://github.com/muffinista/lists_of_lists"&gt;on
github&lt;/a&gt; – please feel
free to take it and adapt it to your needs!&lt;/p&gt;

</content>
  </entry>
  <entry>
    <title>@lists_of_lists</title>
    <link rel="alternate" href="https://twitter.com/lists_of_lists"/>
    <id>https://twitter.com/lists_of_lists</id>
    <published>2016-08-05T00:00:00+00:00</published>
    <updated>2016-08-05T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">Twitter bot that tweets lists found on Wikipedia. Source code is &lt;a href='https://github.com/muffinista/lists_of_lists'&gt;here&lt;/a&gt;.</content>
  </entry>
  <entry>
    <title>@HulkDonaldTrump</title>
    <link rel="alternate" href="https://twitter.com/HulkDonaldTrump"/>
    <id>https://twitter.com/HulkDonaldTrump</id>
    <published>2016-08-04T00:00:00+00:00</published>
    <updated>2016-08-04T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">Twitter bot that tweets like the Hulk version of Donald Trump. Here's the &lt;a href='https://github.com/muffinista/HulkDonaldTrump'&gt;source code&lt;/a&gt;.</content>
  </entry>
  <entry>
    <title>Tweet Masker</title>
    <link rel="alternate" href="https://chrome.google.com/webstore/detail/tweet-masker/aobdgenfpejjjfcpagkhognobonnjcbc"/>
    <id>https://chrome.google.com/webstore/detail/tweet-masker/aobdgenfpejjjfcpagkhognobonnjcbc</id>
    <published>2016-06-10T00:00:00+00:00</published>
    <updated>2016-06-10T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">Chrome extension that can mask tweets that have content warnings.</content>
  </entry>
  <entry>
    <title>@kiki_flies_exe</title>
    <link rel="alternate" href="https://twitter.com/kiki_flies_exe"/>
    <id>https://twitter.com/kiki_flies_exe</id>
    <published>2016-05-01T00:00:00+00:00</published>
    <updated>2016-05-01T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">a bot that outputs scenes from Kiki's Delivery Service.</content>
  </entry>
  <entry>
    <title>@cat_in_field</title>
    <link rel="alternate" href="https://twitter.com/cat_in_field"/>
    <id>https://twitter.com/cat_in_field</id>
    <published>2016-03-07T00:00:00+00:00</published>
    <updated>2016-03-07T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">a cat playing in a field on Twitter. Source code is at &lt;a href='https://github.com/muffinista/cat_in_field'&gt;https://github.com/muffinista/cat_in_field&lt;/a&gt;</content>
  </entry>
  <entry>
    <title>@snowfall_exe</title>
    <link rel="alternate" href="https://twitter.com/snowfall_exe"/>
    <id>https://twitter.com/snowfall_exe</id>
    <published>2015-12-14T00:00:00+00:00</published>
    <updated>2015-12-14T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">a bot that will turn an image into an animated GIF with falling snow. Source code is at &lt;a href='https://github.com/muffinista/snowfall_exe'&gt;https://github.com/muffinista/snowfall_exe&lt;/a&gt;</content>
  </entry>
  <entry>
    <title>emoji yule log</title>
    <link rel="alternate" href="http://muffinlabs.com/emoji_yule_log/"/>
    <id>http://muffinlabs.com/emoji_yule_log/</id>
    <published>2015-12-05T00:00:00+00:00</published>
    <updated>2015-12-05T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html"></content>
  </entry>
  <entry>
    <title>@yulelogbot</title>
    <link rel="alternate" href="https://twitter.com/yulelogbot"/>
    <id>https://twitter.com/yulelogbot</id>
    <published>2015-12-05T00:00:00+00:00</published>
    <updated>2015-12-05T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">an emoji-powered fireplace on twitter, tweeting every 30 minutes through the holiday season.</content>
  </entry>
  <entry>
    <title>@wayback_exe</title>
    <link rel="alternate" href="http://muffinlabs.com/wayback_exe/"/>
    <id>http://muffinlabs.com/wayback_exe/</id>
    <published>2015-10-15T00:00:00+00:00</published>
    <updated>2015-10-15T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html"></content>
  </entry>
  <entry>
    <title>@botgle</title>
    <link rel="alternate" href="http://muffinlabs.com/botgle/"/>
    <id>http://muffinlabs.com/botgle/</id>
    <published>2015-07-01T00:00:00+00:00</published>
    <updated>2015-07-01T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html"></content>
  </entry>
  <entry>
    <title>@kaleid_o_bot</title>
    <link rel="alternate" href="http://muffinlabs.com/kaleid_o_bot/"/>
    <id>http://muffinlabs.com/kaleid_o_bot/</id>
    <published>2015-02-01T00:00:00+00:00</published>
    <updated>2015-02-01T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html"></content>
  </entry>
  <entry>
    <title>Chatterbot: A Ruby Library for Twitter Bots</title>
    <link rel="alternate" href="http://muffinlabs.com/chatterbot.html"/>
    <id>http://muffinlabs.com/chatterbot.html</id>
    <published>2015-01-01T00:00:00+00:00</published>
    <updated>2015-01-01T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html"></content>
  </entry>
  <entry>
    <title>A Real River</title>
    <link rel="alternate" href="http://muffinlabs.com/2014/12/11/a-real-river/"/>
    <id>http://muffinlabs.com/2014/12/11/a-real-river/</id>
    <published>2014-12-11T00:25:00+00:00</published>
    <updated>2014-12-11T00:25:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">&lt;p&gt;&lt;a href="https://twitter.com/arealriver"&gt;@ARealRiver&lt;/a&gt; is a Twitter bot that charts the course of a
generative river via emoji. The course of the river is constant as it
transitions between tweets, so you can scroll through 100s of tweets
and watch the river expand and shrink, and meander back and forth,
passing cities and forests and volcanoes and other scenery as it goes.&lt;/p&gt;

&lt;p&gt;&lt;img src="/images/ARealRiver.png" /&gt;&lt;/p&gt;

&lt;p&gt;There were a lot of different inspirations for this bot. I was
directly influenced by &lt;a href="https://twitter.com/katierosepipkin"&gt;@katierosepipkin&lt;/a&gt;'s
&lt;a href="https://twitter.com/tiny_star_field"&gt;@tiny_star_field&lt;/a&gt;, &lt;a href="https://twitter.com/dungeon_bot"&gt;dungeon_bot&lt;/a&gt; by
&lt;a href="https://twitter.com/jeffthompson_"&gt;@jeffthompson_&lt;/a&gt;, as well as by accounts like
&lt;a href="https://twitter.com/crashtxt"&gt;@crashtxt&lt;/a&gt; and the &lt;a href="https://twitter.com/hashtag/140art?src=hash"&gt;#140art&lt;/a&gt; hash tag.&lt;/p&gt;

&lt;p&gt;Another lingering inspiration was a book from the early 80s: &lt;em&gt;Computer
Spacegames&lt;/em&gt; from Usborne Publishing.&lt;/p&gt;

&lt;p&gt;&lt;a href="/images/computer-spacegames.jpg"&gt;&lt;img src="/images/computer-spacegames-small.jpg" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This book was one of several that introduced me to programming. You
can get a look at it and many others like it &lt;a href="http://mocagh.org/loadpage.php?getcompany=usborne-hayes"&gt;here&lt;/a&gt;. It's
full of source code for simple games written in BASIC. In particular,
there's one called &lt;em&gt;Death Valley&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="/images/death-valley.jpg"&gt;&lt;img src="/images/death-valley-small.jpg" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This was a super-simple game that placed you in a canyon that probably
looked a lot like this:&lt;/p&gt;

&lt;pre&gt;
*            *
 *            *
  *            *
   *            *
    *            *
     *            *
   *            *
    *            *
  *            *
 *            *
*     X       *
&lt;/pre&gt;

&lt;p&gt;Your ship is the X, and you need to run along the canyon for as long
as possible. Good luck!&lt;/p&gt;

&lt;p&gt;I spent years iterating on programs like this as a young programmer,
all the way through high school. I would experiment with different
output, different speeds, obstacles, etc. It's always stuck with me
and ARealRiver is definitely inspired by my time with this code.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <title>EarthRoverBot</title>
    <link rel="alternate" href="http://muffinlabs.com/rover/"/>
    <id>http://muffinlabs.com/rover/</id>
    <published>2014-10-01T00:00:00+00:00</published>
    <updated>2014-10-01T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html"></content>
  </entry>
  <entry>
    <title>US Prisons</title>
    <link rel="alternate" href="http://muffinlabs.com/2014/09/13/us-prisons/"/>
    <id>http://muffinlabs.com/2014/09/13/us-prisons/</id>
    <published>2014-09-13T13:14:00+00:00</published>
    <updated>2014-09-13T13:14:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">&lt;p&gt;I launched &lt;a href="https://twitter.com/usprisons"&gt;@USPrisons&lt;/a&gt; on Twitter a
few weeks ago. It will output every prison in the
United States – or at least, I think it's all of them. I found a
&lt;a href="http://www.insideprison.com/"&gt;website&lt;/a&gt; with the data, did a bunch of parsing and cleanup, and ended
up with 4763 prisons. The bot should spend a year listing them all,
along with a few stats, and a picture if possible.&lt;/p&gt;

&lt;p&gt;If you're interested, I released
&lt;a href="https://github.com/muffinista/prison_scrape"&gt;the code&lt;/a&gt; that does the
parsing on github.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <title>StckMrktStatus - Providing Logical Explanations for the Stock Market</title>
    <link rel="alternate" href="http://muffinlabs.com/2014/04/21/stckmrktstatus-providing-logical-explanations-for-the-stock-market/"/>
    <id>http://muffinlabs.com/2014/04/21/stckmrktstatus-providing-logical-explanations-for-the-stock-market/</id>
    <published>2014-04-21T00:00:00+00:00</published>
    <updated>2014-04-21T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">&lt;p&gt;I've always thought the stock market reports you hear on the news are
fairly silly. "The Dow Jones was up x% because this or that happened."
The people saying those things always sound smart and informed, but no
one really has any idea why a stock goes up or down in value. So, I
made a bot to do the same thing. &lt;a href="https://twitter.com/StckMrktStatus"&gt;@StckMrktStatus&lt;/a&gt; will pick a stock
from the NASDAQ or Dow Jones, see how it is doing for the day, and
then add a reason for the change. The reasons are pulled from tweets
that have the word 'because' on them. It's pretty simple but seems to
work nicely:&lt;/p&gt;

&lt;p&gt;&lt;a class="twitter-timeline" href="https://twitter.com/StckMrktStatus" data-widget-id="458347021812789248"&gt;Tweets by @StckMrktStatus&lt;/a&gt;
&lt;script&gt;!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The code is pretty simple, and I'll post it sometime soon (I'm working
on a post about the code of my last few bots in general).&lt;/p&gt;

</content>
  </entry>
  <entry>
    <title>SpaceJamCheck: Space Jam website monitoring on Twitter</title>
    <link rel="alternate" href="http://muffinlabs.com/2014/01/09/spacejamcheck-space-jam-website-monitoring-on-twitter/"/>
    <id>http://muffinlabs.com/2014/01/09/spacejamcheck-space-jam-website-monitoring-on-twitter/</id>
    <published>2014-01-09T00:00:00+00:00</published>
    <updated>2014-01-09T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">&lt;p&gt;People who have been online for awhile probably know that the &lt;a href="http://www2.warnerbros.com/spacejam/movie/jam.htm"&gt;website
for Space Jam&lt;/a&gt;, a movie from 1996, is online still, and is essentially
unchanged:&lt;/p&gt;

&lt;p&gt;&lt;img src="/images/space-jam-small.jpg" /&gt;&lt;/p&gt;

&lt;p&gt;(If you don't know what I'm talking about, you can read about it &lt;a href="http://lmgtfy.com/?q=space+jam+website+still+up"&gt;here&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;At the end of 2010, someone noticed that the website was still online.
Before I did a little research, I was convinced that people must have
realized this before then, but &lt;a href="http://www.google.com/trends/explore#q=%22space%20jam%22&amp;amp;geo=US&amp;amp;cmpt=q"&gt;Google suggests otherwise&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, here's &lt;a href="http://techcrunch.com/2010/12/31/space-jam/"&gt;an article&lt;/a&gt; that summarizes how it all happened,
basically some Reddit user noticed, the word spread, and then it went
viral on Twitter.&lt;/p&gt;

&lt;p&gt;I haven't seen this mentioned anywhere, but according to the headers
for the website, there were actually some modifications of some sort
in 2005:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HEAD http://www2.warnerbros.com/spacejam/movie/jam.htm
200 OK
Connection: close
Date: Fri, 10 Jan 2014 02:12:09 GMT
Accept-Ranges: bytes
ETag: "89dfb-13c5-4027752a8ca80"
Server: Apache
Content-Length: 5061
Content-Type: text/html
Last-Modified: Thu, 06 Oct 2005 15:10:18 GMT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It's possible this was just a server move or something like that, but
it's interesting to think that someone actually did some &lt;em&gt;maintenance&lt;/em&gt;
of some sort on the site.&lt;/p&gt;

&lt;p&gt;I enjoy visiting the site, especially when I get nostalgic for the
early days of my work on the internet. There are so many projects
which I've worked on over the years, and a lot of them are gone
forever. It's nice to see one that has managed to survive.&lt;/p&gt;

&lt;p&gt;Because I'm lazy, and like easy reassurance, I wrote a
&lt;a href="https://twitter.com/SpaceJamCheck"&gt;@SpaceJamStatus&lt;/a&gt;, a Twitter bot that will check on the status of the
website every few hours and tweet out the status:&lt;/p&gt;

&lt;p&gt;&lt;a class="twitter-timeline" href="https://twitter.com/SpaceJamCheck" data-widget-id="421465296361103361"&gt;Tweets by @SpaceJamCheck&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Furthermore, because I am apocalyptic, I wrote
&lt;a href="https://twitter.com/spacejamisdown"&gt;@spacejamisdown&lt;/a&gt;, a bot which checks the status of
the website every few hours, and will only report if it's not online:&lt;/p&gt;

&lt;p&gt;&lt;a class="twitter-timeline" href="https://twitter.com/spacejamisdown" data-widget-id="421465631746060288"&gt;Tweets by @spacejamisdown&lt;/a&gt;
&lt;script&gt;!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;With a little luck, this bot won't tweet any time soon.&lt;/p&gt;

&lt;p&gt;Finally, because I have a love of writing random libraries, I wrote
the ruby gem &lt;a href="https://github.com/muffinista/spacejam"&gt;spacejam&lt;/a&gt;, which is a pretty simple Ruby library
you can use to check on the status of any website. It can do tests
against expected response codes, the body of a page, etc. It's pretty
simple, but it's good enough to check on the status of the Space Jam
website.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <title>Each Town - Listing All Towns in America on Twitter</title>
    <link rel="alternate" href="http://muffinlabs.com/2013/10/16/each-town-listing-all-towns-in-america-on-twitter/"/>
    <id>http://muffinlabs.com/2013/10/16/each-town-listing-all-towns-in-america-on-twitter/</id>
    <published>2013-10-16T00:00:00+00:00</published>
    <updated>2013-10-16T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">&lt;p&gt;A week or two ago I launched &lt;a href="https://twitter.com/eachtown"&gt;@eachtown&lt;/a&gt; on Twitter. It will spend the
next couple years tweeting the name and location of every populated
place in America, in alphabetical order.&lt;/p&gt;

&lt;p&gt;&lt;a class="twitter-timeline" data-dnt="true" href="https://twitter.com/eachtown" data-widget-id="390546604588924928"&gt;Tweets by @eachtown&lt;/a&gt;
&lt;script&gt;!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;A couple of years ago, I spent a lot of time fiddling with the
&lt;a href="http://gnis.usgs.gov/index.html"&gt;USGS database of Geographic Names&lt;/a&gt;. It's a cool set of data and
I've often thought of doing more with it. I was inspired by
&lt;a href="https://twitter.com/everyword"&gt;@everyword&lt;/a&gt; to create something similar, and decided to
create a bot which iterates through every populated place in America,
and tweets the name, and a link to a Google Map for the location. I
enjoy the context you get from having the ability to look at a place.
Not every location in the database is a city or even a town. There's
mobile home parks, condominiums, etc. Seeing them on the map gives you
a sense of the fact that these places are real, and gives them a
little context.&lt;/p&gt;

&lt;div&gt;
&lt;img src="/images/agnew-mobile-home-park-wa.jpg" class="imgp_img" alt="Agnew Mobile Home Park, WA" /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://maps.google.com/?t=k&amp;amp;q=48.122838,-123.221415"&gt;Agnew Mobile Home Park, WA&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It's a pretty simple bot, and I'll post the source code at some point
once I clean it up a little.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <title>Gopherpedia - The Free Encyclopedia via gopher</title>
    <link rel="alternate" href="http://muffinlabs.com/2013/06/14/gopherpedia-the-free-encyclopedia-via-gopher/"/>
    <id>http://muffinlabs.com/2013/06/14/gopherpedia-the-free-encyclopedia-via-gopher/</id>
    <published>2013-06-14T00:00:00+00:00</published>
    <updated>2013-06-14T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">&lt;p&gt;My last release for Project Dump week is &lt;a href="http://gopherpedia.com/"&gt;Gopherpedia&lt;/a&gt; –
a mirror of Wikipedia in gopherspace. If you happen to have a gopher
client, you can see it at gopherpedia.com on port 70. Otherwise, you
can browse to &lt;a href="http://gopherpedia.com/"&gt;gopherpedia.com&lt;/a&gt; and view it via a web
proxy.&lt;/p&gt;

&lt;p&gt;A couple of years ago, I landed on the idea of a gopher interface to
Wikipedia. Originally it was probably a joke, but it stuck with me. So
one day I registered a domain name and got to work. The first thing I
needed to do was build a gopher server, because none of the currently
available options were up to the task. So I built
&lt;a href="https://github.com/muffinista/gopher2000"&gt;Gopher2000&lt;/a&gt;. Then, I quickly realized that the current
gopher proxies weren't any good either, so I built &lt;a href="https://github.com/muffinista/gophper-proxy"&gt;GoPHPer&lt;/a&gt;.
Once both of those were written (well over a year ago), it didn't seem
like there was much left to be done – gopherpedia should've been
ready to launch.&lt;/p&gt;

&lt;p&gt;But I hadn't reckoned on the challenges of churning through a database dump
of Wikipedia.&lt;/p&gt;

&lt;p&gt;Wikipedia is very open. They have an API which you can use to search
and query documents, and they provide
&lt;a href="http://dumps.wikimedia.org/"&gt;downloadable archives&lt;/a&gt; of their entire collection of
databases. They encourage you to download these, mirror them, etc.&lt;/p&gt;

&lt;p&gt;My first implementation of gopherpedia used the API. This worked well,
but had two problems. First, it was a little slow, since it needed to
query a remote server for every request. Second, Wikipedia prohibits
using the API this way - if you want to make a mirror of their
website, they want you to download an archive and use that, so their
servers aren't overloaded.&lt;/p&gt;

&lt;p&gt;So I downloaded a dump of their database, which is a single 9GB
compressed XML file. Nine. Gigabytes. Compressed. A single file.&lt;/p&gt;

&lt;p&gt;Then a took the opportunity to learn about streaming XML Parsers.
Basically I wrote a &lt;a href="https://gist.github.com/muffinista/5781615"&gt;parser script&lt;/a&gt; that parsed the file while
it was reading it, as opposed to reading the whole thing into memory
at once, which was clearly impossible. The script splits up wikipedia
entries and stores them as flat text files. Running that script took a
couple days on my extremely cheap Dreamhost server – that's right, I
have a gopher server hosted on Dreamhost.&lt;/p&gt;

&lt;p&gt;So, when someone requests a page, the gopher server reads that file,
does some parsing, and returns the result as a gopher query. Sounds
simple, right? Not quite, because parsing the contents of a wikipedia
entry is also a mess. It's part wikitext, part HTML, and there's
plenty of places where both are broken. If I was just outputting HTML,
I could probably get away with it. But since this is Gopher I really
needed to format the results as plain text. I spent a while writing an
incredibly messy parser, and the imperfect results are what you see on
gopherpedia now. Sorry for all the flaws.&lt;/p&gt;

&lt;p&gt;Anyway, this was a fun project, and it occupied a pleasant chunk of my
spare time over the last year or two, but it's time to release it to
the wild. Unless I'm mistaken, this is now the largest gopher site in
existence. There are about 4.2 million pages on gopherpedia, totaling
somewhere over 10GB of data.&lt;/p&gt;

&lt;p&gt;Here's my favorite page on the site – the
&lt;a href="http://gopherpedia.com/gopherpedia.com/Gopher%20(protocol)"&gt;gopherified wikipedia entry for Gopher&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Please note, this is in extreme beta, and is likely to break, just let
me know if you have any problems. Enjoy!&lt;/p&gt;

</content>
  </entry>
  <entry>
    <title>Gophper - A Modern Gopher Proxy for the Modern Age</title>
    <link rel="alternate" href="http://muffinlabs.com/2013/06/13/gophper/"/>
    <id>http://muffinlabs.com/2013/06/13/gophper/</id>
    <published>2013-06-13T00:00:00+00:00</published>
    <updated>2013-06-13T00:00:00+00:00</updated>
    <author>
      <name>Colin Mitchell</name>
    </author>
    <content type="html">&lt;p&gt;As I mentioned &lt;a href="http://muffinlabs.com/2013/06/12/goper2000---a-modern-gopher-server"&gt;yesterday&lt;/a&gt;, building Gopher applications
is fun, but using gopherspace is actually pretty challenging unless
you're a die-hard throwback geek. I have a super-secret gopher project
(to be revealed tomorrow), but it's pretty useless if no one can
actually see it. Sure, I could write up a blog post about how to
download a gopher client, etc, etc, but that's just dumb.&lt;/p&gt;

&lt;p&gt;There's a few gopher proxies out there – primarily
&lt;a href="http://gopher.floodgap.com/gopher/"&gt;floodgap&lt;/a&gt; and &lt;a href="http://gopherproxy.meulie.net/"&gt;meulie&lt;/a&gt; – these are websites which
you can use to browse gopher servers. But there's a few problems with
these proxies. First, they're a little clunky. They're handy tools,
but they're not really attractive, and the HTML they output is pretty
old-fashioned. And most importantly, neither one is open-source.&lt;/p&gt;

&lt;p&gt;I wanted a simple gopher proxy, using modern web standards, that was
open-source and easy to install. So, I wrote &lt;a href="https://github.com/muffinista/gophper-proxy"&gt;gophper&lt;/a&gt;.
You can see it in action at &lt;a href="http://gopher.muffinlabs.com/"&gt;gopher.muffinlabs.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here's the details:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It runs on PHP using &lt;a href="http://www.slimframework.com/"&gt;Slim&lt;/a&gt;, which is a nifty lightweight application framework.&lt;/li&gt;
  &lt;li&gt;It caches requests for faster response times.&lt;/li&gt;
  &lt;li&gt;All of the rendering happens in the browser, which means someone could easily write a different backend.&lt;/li&gt;
  &lt;li&gt;It has a wacky theme switcher, so you can choose between a nice
modern look, or an old-school monochrome CRT look.&lt;/li&gt;
  &lt;li&gt;If the user accesses a binary file, they can download it. If they click on an image, they can see it in the browser.&lt;/li&gt;
  &lt;li&gt;It can be integrated with Google Analytics.&lt;/li&gt;
  &lt;li&gt;You can restrict it to a single gopher server, so you can integrate it into your project without any fears of someone using your proxy for naughty tricks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It's still a little rough around the edges, but it definitely works. I
would love to see it used all over the place. But tomorrow I'll reveal
where I'm using it.&lt;/p&gt;

</content>
  </entry>
</feed>
